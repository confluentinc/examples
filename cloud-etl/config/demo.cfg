##########################################################
# Confluent Cloud
##########################################################

# Confluent Cloud configuration file
# - File should include the following configuration parameters
# bootstrap.servers=<BROKER ENDPOINT>
# ssl.endpoint.identification.algorithm=https
# security.protocol=SASL_SSL
# sasl.mechanism=PLAIN
# sasl.jaas.config=org.apache.kafka.common.security.plain.PlainLoginModule required username\="<API KEY>" password\="<API SECRET>";
# schema.registry.url=https://<SR ENDPOINT>
# schema.registry.basic.auth.user.info=<SR API KEY>:<SR API SECRET>
# basic.auth.credentials.source=USER_INFO
# ksql.endpoint=https://<KSQL ENDPOINT>
# ksql.basic.auth.user.info=<KSQL API KEY>:<KSQL KSQL SECRET>
CONFIG_FILE=~/.ccloud/config


##########################################################
# AWS Kinesis
##########################################################

# KINESIS_STREAM_NAME: will be created and deleted by the demo
export KINESIS_STREAM_NAME='demo-logs'

export KINESIS_REGION='us-west-2'

# AWS_PROFILE: profile must exist in ~/.aws/credentials
export AWS_PROFILE=default


##########################################################
# Cloud storage sink
##########################################################

export STORAGE_REGION='us-west-2'

# DESTINATION_STORAGE can be one of 's3' or 'gcs' or 'az'
export DESTINATION_STORAGE='gcs'

# The parameters below need to be set depending on which storage cloud provider you set in DESTINATION_STORAGE

##########################################################
# AWS S3
##########################################################

# S3_PROFILE: profile must exist in ~/.aws/credentials
#export S3_PROFILE=default

# S3_BUCKET: bucket name
# Demo will modify contents of the bucket
# Do not specify one that you do not want accidentally deleted
#export S3_BUCKET='confluent-cloud-demo'

##########################################################
# GCP GCS
##########################################################

# GCS_CREDENTIALS_FILE: json-formatted file with credentials, downloaded from the Google Cloud Console
# File should resemble example in https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating_service_account_keys
export GCS_CREDENTIALS_FILE=~/.config/gcloud/cloud-private.json

# GCS_BUCKET: bucket name
# Demo will modify contents of the bucket
# Do not specify one that you do not want accidentally deleted
export GCS_BUCKET='confluent-cloud-demo'

##########################################################
# Azure Blob 
##########################################################

# AZBLOB_STORAGE_ACCOUNT: name of storage account must exist, with keys
#export AZBLOB_STORAGE_ACCOUNT=demorg23abc

# AZBLOB_CONTAINER: container name
# Demo will modify contents of the container
# Do not specify one that you do not want accidentally deleted
#export AZBLOB_CONTAINER='confluent-cloud-demo'


##########################################################
# Kafka topic names
##########################################################

# Do not modify topic names
# These must stay in sync with the ksql.commands and ksql.cleanup.commands files
export KAFKA_TOPIC_NAME_IN='eventLogs'
export KAFKA_TOPIC_NAME_OUT1='SUM_PER_SOURCE'
export KAFKA_TOPIC_NAME_OUT2='COUNT_PER_SOURCE'
