##########################################################
# Source
##########################################################

# DATA_SOURCE can be one of 'kinesis' or 'rds' (Amazon RDS for PostgreSQL)
export DATA_SOURCE='kinesis'

# The parameters below need to be set depending on which data source you set in DATA_SOURCE

#---------------------------------------------------------
# AWS Kinesis
#---------------------------------------------------------

# KINESIS_STREAM_NAME: will be created and deleted by the demo
#export KINESIS_STREAM_NAME='demo-logs'

#export KINESIS_REGION='us-west-2'

# AWS_PROFILE: profile must exist in ~/.aws/credentials
#export AWS_PROFILE=default

#---------------------------------------------------------
# AWS RDS for PostgreSQL
#---------------------------------------------------------

# DB_INSTANCE_IDENTIFIER: PostgreSQL DB will be created and deleted by the demo
#export DB_INSTANCE_IDENTIFIER=confluentdemo

#export RDS_REGION='us-west-2'

# AWS_PROFILE: profile must exist in ~/.aws/credentials
#export AWS_PROFILE=default



##########################################################
# Cloud storage sink
##########################################################

export STORAGE_REGION='us-west-2'

# DESTINATION_STORAGE can be one of 's3' or 'gcs' or 'az'
export DESTINATION_STORAGE='s3'

# The parameters below need to be set depending on which storage cloud provider you set in DESTINATION_STORAGE

#---------------------------------------------------------
# AWS S3
#---------------------------------------------------------

# S3_PROFILE: profile must exist in ~/.aws/credentials
#export S3_PROFILE=default

# S3_BUCKET: bucket name
# Demo will modify contents of the bucket
# Do not specify one that you do not want accidentally deleted
#export S3_BUCKET='cloud-etl-example'

#---------------------------------------------------------
# GCP GCS
#---------------------------------------------------------

# GCS_CREDENTIALS_FILE: json-formatted file with credentials, downloaded from the Google Cloud Console
# File should resemble example in https://cloud.google.com/iam/docs/creating-managing-service-account-keys#creating_service_account_keys
#export GCS_CREDENTIALS_FILE=~/.config/gcloud/cloud-private.json

# GCS_BUCKET: bucket name
# Demo will modify contents of the bucket
# Do not specify one that you do not want accidentally deleted
#export GCS_BUCKET='confluent-cloud-etl-demo'

#---------------------------------------------------------
# Azure Blob 
#---------------------------------------------------------

# AZBLOB_STORAGE_ACCOUNT: name of storage account must exist, with keys
#export AZBLOB_STORAGE_ACCOUNT=demorg23abc

# AZBLOB_CONTAINER: container name
# Demo will modify contents of the container
# Do not specify one that you do not want accidentally deleted
#export AZBLOB_CONTAINER='confluent-cloud-etl-demo'



##########################################################
# Kafka topic names
##########################################################

# Do not modify topic names
# These must stay in sync with the statements.sql and cleanup_statements.sql files
export KAFKA_TOPIC_NAME_IN='eventlogs'
export KAFKA_TOPIC_NAME_OUT1='SUM_PER_SOURCE'
export KAFKA_TOPIC_NAME_OUT2='COUNT_PER_SOURCE'
