= Live Coding the KSQL Music demo
:source-highlighter: pygments
:doctype: book
v1.00, 4 Sept 2018

:toc:

== Introduction

In this tutorial we will run Confluent’s Kafka Music demo application using KSQL. The Kafka Music application demonstrates how to build a simple music charts application that continuously computes, in real-time, the latest music charts.

image::images/ksql-music-demo-overview.jpg[Diagram]

You may separately compare this KSQL demo to the https://docs.confluent.io/current/streams/kafka-streams-examples/docs/index.html[Kafka Streams API version of the demo] if you want to see the differences.

This code file accompanies:

- https://www.youtube.com/watch?v=ExEWJVjj-RA[video screencast]
- https://github.com/confluentinc/quickstart-demos/tree/5.0.0-post/music[automated demo]

Don't forget to check out the #ksql channel on our https://slackpass.io/confluentcommunity[Community Slack group]

== Setup

=== Docker

This tutorial assumes you are running the KSQL music demo application in Docker. 

NOTE: You may alternatively run this demo on a Confluent Platform local install, just by running the https://github.com/confluentinc/quickstart-demos/blob/5.0.0-post/music/start.sh[start script].

To get this demo running in Docker, clone the https://github.com/confluentinc/quickstart-demos[repo] with the KSQL music demo.

[source,bash]
----
$ git clone https://github.com/confluentinc/quickstart-demos
----

In Docker's advanced https://docs.docker.com/docker-for-mac/#advanced[settings], increase the memory dedicated to Docker to at least 8GB (default is 2GB).

From the `quickstart-demos/music` directory, start the demo by running a single command that brings up the Docker containers.  This will take less than 2 minutes to complete.

[source,bash]
----
$ docker-compose up -d
----

Do not proceed until you see the output in the logs below.

[source,bash]
----
$ docker-compose logs -f control-center | grep -i "Started NetworkTrafficServerConnector"
control-center                | [2018-09-06 15:03:22,518] INFO Started NetworkTrafficServerConnector@4d48bd85{HTTP/1.1,[http/1.1]}{0.0.0.0:9021} (org.eclipse.jetty.server.AbstractConnector)
----

== Demo

=== Explore the source data

The demo auto-generates source data to two topics, in Avro format:

* `play-events` : stream of play events (“song X was played”)
* `song-feed` : stream of song metadata (“song X was written by artist Y”)

Use Google Chrome to navigate to Confluent Control Center and inspect the Kafka topics: http://localhost:9021/development/ksql/.

If you don't want to use the KSQL UI in Control Center, use the KSQL CLI:

[source,bash]
----
$ docker-compose exec ksql-cli ksql http://ksql-server:8088
----

* Exploring the topic `play-events` : 

image:images/topic_inspect_play_events.png[play-events]

From KSQL CLI:

[source,sql]
----
ksql> print "play-events";
9/4/18 10:20:01 AM EDT, uk, {"song_id": 21, "duration": 60000}
9/4/18 10:20:02 AM EDT, uk, {"song_id": 15, "duration": 60000}
9/4/18 10:20:02 AM EDT, uk, {"song_id": 1, "duration": 60000}
9/4/18 10:20:02 AM EDT, uk, {"song_id": 3, "duration": 60000}
....
----

* Exploring the topic `song-feed` : 

At this time, you cannot view data in this topic using the topic inspection capability of Confluent Control Center because topic inspection only works on new data, not old data already produced to the topic.  Instead, use KSQL CLI to view the `song-feed` topic:

From KSQL CLI:

[source,sql]
----
ksql> print "song-feed" from beginning;
9/4/18 10:10:43 AM EDT, , {"id": 1, "album": "Fresh Fruit For Rotting Vegetables", "artist": "Dead Kennedys", "name": "Chemical Warfare", "genre": "Punk"}
9/4/18 10:10:43 AM EDT, , {"id": 2, "album": "We Are the League", "artist": "Anti-Nowhere League", "name": "Animal", "genre": "Punk"}
9/4/18 10:10:43 AM EDT, , {"id": 3, "album": "Live In A Dive", "artist": "Subhumans", "name": "All Gone Dead", "genre": "Punk"}
9/4/18 10:10:43 AM EDT, , {"id": 4, "album": "PSI", "artist": "Wheres The Pope?", "name": "Fear Of God", "genre": "Punk"}
....
----

=== Create a Stream from the `play-events` topic

To process the data in Kafka, let's begin with the `play-events` topic because it’s more straightforward.

NOTE: we prefix the KSQL stream, table,a nd query names with `ksql_` but that is not required. We are doing it so that you can run these KSQL queries alongside the Kafka Streams API version of this music demo and not run into naming conflicts.

We need to register the topic `play-events` as a KSQL stream, and specify that it’s Avro data.

* Configure the stream name as `ksql_playevents`
* Change the message value encoding to `AVRO` (default: `JSON`)
* Notice that because of Control Center integration with Schema Registry, it automatically gleans the fields and types for `song_id` and `duration` in the payload

image:images/ksql_playevents.png[play-events]

From KSQL CLI:

[source,sql]
----
ksql> CREATE STREAM ksql_playevents WITH (KAFKA_TOPIC='play-events', VALUE_FORMAT='AVRO');
----

=== Filter data

Do some basic filtering, e.g. to qualify songs that were played for at least 30 seconds.

If you are running Confluent Enterprise 5.0 or later, use the integrated KSQL capabilities in Control Center.  From the query editor:

image:images/ksql_playevents_min_30_non_persistent.png[play-events]

The above query is not persistent -- it will stop if this screen is closed. To make the query persistent and stay running until explicitly terminated, prefix the previous query with `CREATE STREAM AS`:

image:images/ksql_playevents_min_30_persistent.png[play-events]

This persistent query will now show in the `PERSISTENT QUERIES` tab (or in KSQL CLI, it will show in the output of `show queries;`).

=== Create a Table from the `song-feed` topic

Next let's work with the generated Kafka data for the `song-feed` topic, which represents a database of songs. The goal is to view this data as a TABLE key’d on song id.

However, the original Kafka topic has no key, i.e., the key of each Kafka message is `null`. To make a KSQL TABLE, we need the topic to have a non-null key for JOINs and aggregations to work.  We can address this in a few simple steps:

* Create a `STREAM` from the original Kafka topic `song-feed`:

[source,sql]
----
ksql> CREATE STREAM ksql_songfeed WITH (KAFKA_TOPIC='song-feed', VALUE_FORMAT='AVRO');
----
 
As mentioned earlier, if you inspect this stream, you will see that ROWKEY is blank.
 
[source,sql]
----
ksql> SELECT * FROM ksql_songfeed limit 5;
----
 
`DESCRIBE` the stream to see the fields associated with this topic, and notice that ID is of type `BIGINT`.
 
[source,sql]
----
ksql> DESCRIBE ksql_songfeed;
----
 
* Observe the following in the newly created stream:

(a) the stream has no key
(b) the ID field that we would want to be the key `ID` is of type `BIGINT`

We need to resolve these two issues because in the current KSQL release, a TABLE is required to have a key and the key is required to be of type String. We can address both of these issues with one command that makes the ID to be of type String using the `CAST` scalar function, and assigns the ID as the key of the STREAM using the `PARTITION BY` clause..
 
[source,sql]
----
ksql> CREATE STREAM ksql_songfeedwithkey WITH (KAFKA_TOPIC='KSQL_SONGFEEDWITHKEY', VALUE_FORMAT='AVRO') AS SELECT CAST(ID AS STRING) as ID, ALBUM, ARTIST, NAME, GENRE FROM ksql_songfeed PARTITION BY ID;
----
 
* Convert the above stream into a table with the `ID` field as its key (which is now of type `String`). This TABLE is a materialized view of events with only the latest value for each key, which represents an up-to-date database of songs.
 
[source,sql]
----
ksql> CREATE TABLE ksql_songtable WITH (KAFKA_TOPIC='KSQL_SONGFEEDWITHKEY', VALUE_FORMAT='Avro', KEY='ID');
----

=== JOIN play events with the database of songs

We can do a STREAM-TABLE join to bring together the stream of play events with the song table. This will result in a new stream of data that shows not only when a particular song is played, but also descriptive song information like song title along with each play event.

[source,sql]
----
CREATE STREAM ksql_songplays AS SELECT plays.SONG_ID AS ID, ALBUM, ARTIST, NAME, GENRE, DURATION, 1 AS KEYCOL FROM ksql_playevents_min_duration plays LEFT JOIN ksql_songtable songtable ON plays.SONG_ID = songtable.ID;
----

Notice the addition of a clause `1 AS KEYCOL.` This creates a new field `KEYCOL` where every row gets a value of 1. `KEYCOL` can be later used in other derived streams and tables to do aggregations on a global basis, not on a per-partition basis. 

=== Create Top Music Charts

You can create a top music chart for all time to see which songs get the most play. We can use the `COUNT` function on the stream `ksql_songplays` that we created above.

[source,sql]
----
CREATE TABLE ksql_songplaycounts AS SELECT ID, NAME, GENRE, KEYCOL, COUNT(*) AS COUNT FROM ksql_songplays GROUP BY ID, NAME, GENRE, KEYCOL;
----

While the all-time greatest hits are cool, we also might not mind knowing the stats just in the last 30 seconds. Create another query, adding in a `WINDOW` clause, which gives counts of play events for all songs, in 30-second intervals.

[source,sql]
----
CREATE TABLE ksql_songplaycounts30 AS SELECT ID, NAME, GENRE, KEYCOL, COUNT(*) AS COUNT FROM ksql_songplays WINDOW TUMBLING (size 30 seconds) GROUP BY ID, NAME, GENRE, KEYCOL;
----

== Jumping Ahead

=== KSQL Command File

For learning purposes, we suggest you walk through this tutorial step-by-step.

However, if you choose to jump ahead to the end state, run the KSQL command file that automatically configures the KSQL queries.

[source,bash]
----
$ docker-compose exec ksql-cli  bash -c "ksql http://ksql-server:8088 <<EOF
run script '/tmp/ksql.commands';
exit ;
EOF
"
----
